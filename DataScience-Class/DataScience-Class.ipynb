{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 805,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import shapiro\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import preprocessing \n",
    "\n",
    "class DataDefinition():\n",
    "    \n",
    "    # General information and statistical data about the data set are accessed.\n",
    "    def __init__(self, dataName):\n",
    "        print(\"dataDefinition init çağrıldı\")\n",
    "        self.dataName = dataName\n",
    "    \n",
    "    \n",
    "    def Information(self,data):\n",
    "        print(\"---------------------Data Head---------------------\")\n",
    "        print(data.head())\n",
    "        print(\"\\n---------------------Data Describe---------------------\")\n",
    "        print(data.describe().T)\n",
    "        print(\"\\n---------------------Data Info---------------------\")\n",
    "        print(data.info())\n",
    "        print(\"\\n---------------------Data Columns---------------------\")\n",
    "        print(data.columns)\n",
    "        print(\"\\n---------------------Null Sayısı---------------------\")\n",
    "        print( data.isnull().sum().sum())\n",
    "        \n",
    "        \n",
    "    def variable_type(self,data):\n",
    "        kat_df = data.select_dtypes(include = [\"object\"])\n",
    "        print(\"\\n---------------------Categorical Variables---------------------\")\n",
    "        print(kat_df)\n",
    "        print(\"\\n---------------------Value Counts---------------------\")\n",
    "        kat_deg = []\n",
    "        for katDeg in kat_df:\n",
    "            print(katDeg,\": \",data[katDeg].value_counts().count())\n",
    "            kat_deg.append(katDeg)\n",
    "        return kat_deg\n",
    "            \n",
    "        \n",
    "    def categorical_variable_frequency(self, df, variables):\n",
    "        print(\"\\n---------------------Categorical variable frequency---------------------\")\n",
    "        for var in variables:\n",
    "            print(df[var].value_counts())\n",
    "            print(\"\\n---------------------\"+ var +\"---------------------\")\n",
    "            print(df[var].value_counts().plot.barh())\n",
    "            \n",
    "        \n",
    "    def missing_value(self, df):\n",
    "        print(\"\\n---------------------Is there any missing value?---------------------\")\n",
    "        print(df.isnull().values.any())\n",
    "        print(\"\\n---------------------Total number of null observations---------------------\")\n",
    "        print(df.isnull().sum().sum())\n",
    "        print(\"\\n---------------------Which variables have null values?---------------------\")\n",
    "        print(df.isnull().sum())\n",
    "        print(\"\\n---------------------Missing values deleted---------------------\")\n",
    "        df = df.dropna(axis=0)\n",
    "        \n",
    "    \n",
    "    def Dummy(self, data, variable):\n",
    "        dms = pd.get_dummies(data[variable])\n",
    "        data =  data.drop([variable], axis = 1, inplace= True)\n",
    "        data = pd.concat([data, dms], axis = 1)\n",
    "        print(\"\\n---------------------Dummy applied to \"+ variable+ \" variable---------------------\")\n",
    "        print(data.head())\n",
    "\n",
    "    \n",
    "    def NormalDistribution(self, data):\n",
    "        print(\"\\n---------------------Normal distribution control---------------------\")\n",
    "        for i in range (len(data.columns)):\n",
    "            if (data.dtypes[i]!=object):   \n",
    "                stat,p = shapiro(df.iloc[:,i])\n",
    "                if (p>0.05):\n",
    "                    print(\"Orneklem Normal (Gaussian) Dagilimdan gelmektedir (Fail to Reject H0)\")\n",
    "                else:\n",
    "                    print(\"Orneklem Normal (Gaussian) Dagilimdan gelmemektedir\")\n",
    "                    \n",
    "                    \n",
    "\n",
    "class DataVisualization():\n",
    "    \n",
    "    # Provides visualization of the data set and target variable.\n",
    "    def __init__(self, dataName):\n",
    "        print(\"DataVisualization init çağrıldı\")\n",
    "        self.dataName = dataName\n",
    "        \n",
    "    \n",
    "    def histogram(self, data, kat_deg):\n",
    "        df = data.drop(kat_deg, axis=1)\n",
    "        df.hist(figsize=(24,16), bins=40, xlabelsize=6, ylabelsize=6)\n",
    "        plt.show()\n",
    "            \n",
    "            \n",
    "    def heatmap(self, data):\n",
    "        df = data.drop(kat_deg, axis=1)\n",
    "        sns.heatmap(df);\n",
    "    \n",
    "\n",
    "class Preprocessing():\n",
    "    \n",
    "    #Deals with outliers in continuous variables\n",
    "    def __init__(self, dataName):\n",
    "        print(\"OutlierObservationAnalysis init çağrıldı\")\n",
    "        self.dataName = dataName\n",
    "    \n",
    "    \n",
    "    def OutlierObservationDelete(self, df_table):\n",
    "        Q1 = df_table.quantile(0.25)\n",
    "        Q3 = df_table.quantile(0.75)\n",
    "        IQR = Q3-Q1\n",
    "        alt_sinir = Q1- 1.5*IQR\n",
    "        ust_sinir = Q3 + 1.5*IQR\n",
    "        aykiri_tf = ((df_table < (alt_sinir)) | (df_table > (ust_sinir)))\n",
    "        new_table = df_table.drop(df_table[aykiri_tf].index, inplace= True)\n",
    "        return new_table\n",
    "    \n",
    "\n",
    "    def Standardizasyon(self, data):\n",
    "        print(\"\\n---------------------Standardization---------------------\")\n",
    "        from sklearn import preprocessing \n",
    "        preprocessing.scale(data)\n",
    "        \n",
    "    \n",
    "    def Normalizasyon(self, data):\n",
    "        print(\"\\n---------------------Normalization---------------------\")\n",
    "        from sklearn import preprocessing \n",
    "        preprocessing.normalize(data)\n",
    "        \n",
    "    \n",
    "    def min_max(self, data):\n",
    "        print(\"\\n---------------------Min-Max Transform---------------------\")\n",
    "        from sklearn.preprocessing import MinMaxScaler\n",
    "        scaler = MinMaxScaler()\n",
    "        scaler.fit(data)\n",
    "        dd = scaler.transform(data)\n",
    "        data = pd.DataFrame(dd, columns=data.columns)\n",
    "        return data\n",
    "        \n",
    "\n",
    "class Correlation():\n",
    "    \n",
    "    def __init__(self, dataName):\n",
    "        print(\"Models init çağrıldı\")\n",
    "        self.dataName = dataName\n",
    "    \n",
    " \n",
    "    def correlation(self, data):\n",
    "        corr = data.corr()\n",
    "        print(\"\\n---------------------Correlation---------------------\")\n",
    "        print(corr)\n",
    "        \n",
    "        num_deg = []\n",
    "        kat_deg = []\n",
    "\n",
    "        for col in data:\n",
    "            if(data.dtypes[col] == \"object\"):\n",
    "                kat_deg.append(col)\n",
    "            else:\n",
    "                num_deg.append(col)\n",
    "        \n",
    "\n",
    "        columns = np.full((corr.shape[0],), True, dtype=bool)\n",
    "        for i in range(corr.shape[0]):\n",
    "            for j in range(i+1, corr.shape[0]):\n",
    "                if corr.iloc[i,j] >= 0.7:\n",
    "                    if columns[j]:\n",
    "                        columns[j] = False\n",
    "                        \n",
    "        selected_columns = data[num_deg].columns[columns]\n",
    "        data = data[selected_columns]\n",
    "        print(\"\\n---------------------Highly correlated variables deleted---------------------\")\n",
    "        print(\"\\n---------------------New Data Shape---------------------\")\n",
    "        print(data.shape)\n",
    "        return data\n",
    "    \n",
    "    \n",
    "    def target_plot(self, data, target):\n",
    "        import scipy.stats as stats\n",
    "        import pylab\n",
    "        print(\"\\n---------------------Plot of target variable---------------------\")\n",
    "        sns.distplot(data[target])\n",
    "        plt.show()\n",
    "        print(\"\\n---------------------Normality of the target variable---------------------\")\n",
    "        stats.probplot(data[target], dist=\"norm\", plot=pylab)   #normal dağılım mı ?\n",
    "        pylab.show()\n",
    "    \n",
    "    \n",
    "class Processes():\n",
    "    \n",
    "    def __init__(self, dataName):\n",
    "        print(\"Processes init çağrıldı\")\n",
    "        self.dataName = dataName\n",
    "    \n",
    "    \n",
    "    def Split(self, df, target):\n",
    "        import statsmodels.api as sm\n",
    "        from sklearn.model_selection import train_test_split, cross_val_score, cross_val_predict\n",
    "        x_df = df.drop([target], axis=1)\n",
    "        y_df  = df[target]\n",
    "\n",
    "        x_train, x_test, y_train, y_test = train_test_split(x_df,y_df, test_size=0.2, random_state=0, shuffle = False)\n",
    "        print(\"x_train: \" + str(x_train.shape) + \" x_test: \" + str(x_test.shape) +\" y_train: \" + str(y_train.shape) +\" y_test: \" + str(y_test.shape) )\n",
    "        return x_train, x_test, y_train, y_test\n",
    "    \n",
    "    \n",
    "    def linear_regression(self, df, target):\n",
    "        import numpy as np\n",
    "        # MULTİPLE LİNEAR REGRESSİON\n",
    "        print(\"\\n---------------------MULTİPLE LİNEAR REGRESSİON---------------------\")\n",
    "        x_train, x_test, y_train, y_test = self.Split(df, target)\n",
    "        lr = LinearRegression()\n",
    "        lr.fit(x_train, y_train)\n",
    "        y_pred = lr.predict(x_test)\n",
    "        print(np.sqrt(mean_squared_error(self.y_test,y_pred)))\n",
    "       \n",
    "    \n",
    "    def PCA_model(self, data, model):\n",
    "        print(\"\\n---------------------PCA Model---------------------\")\n",
    "        from sklearn.decomposition import PCA\n",
    "        from sklearn.preprocessing import scale\n",
    "        from sklearn.linear_model import LinearRegression\n",
    "        from sklearn.metrics import mean_squared_error, r2_score\n",
    "        \n",
    "        pca = PCA()\n",
    "        x_train, x_test, y_train, y_test = self.Split(data, target)\n",
    "        X_reduced_train = pca.fit_transform(scale(X_train))\n",
    "        print(\"X_reduced_train.shape: \" + X_reduced_train.shape)\n",
    "        \n",
    "        #componentlerin açıklama oranlarının grafiği\n",
    "        import matplotlib.pyplot as plt\n",
    "        features = range(pca.n_components_)\n",
    "        plt.bar(features, pca.explained_variance_ratio_, color='black')\n",
    "        plt.xlabel('PCA features')\n",
    "        plt.ylabel('variance %')\n",
    "        plt.xticks(features);\n",
    "        \n",
    "        \n",
    "        # veri seti içerisindeki değişkenliği açıklama oranları\n",
    "        print(np.cumsum(np.round(pca.explained_variance_ratio_, decimals = 4)*100)[0:15])\n",
    "        \n",
    "        lm = LinearRegression()\n",
    "        pca_model = lm.fit(X_reduced_train, y_train)\n",
    "        y_pred = pca_model.predict(X_reduced_train)\n",
    "        print(\"r2: \" + str(r2_score(y_train, y_pred)))\n",
    "    \n",
    "    \n",
    "    def lojistik_regresyon(self, data, model):\n",
    "        print(\"\\n---------------------Logistic Regression---------------------\")\n",
    "        from sklearn.linear_model import LogisticRegression\n",
    "        y = data[target]\n",
    "        X = data.drop([target], axis=1)\n",
    "        \n",
    "        loj = sm.Logit(y, X)\n",
    "        loj_model= loj.fit()\n",
    "        print(loj_model.summary())\n",
    "        \n",
    "        loj = LogisticRegression(solver = \"liblinear\")\n",
    "        loj_model = loj.fit(X,y)\n",
    "        print(loj_model)\n",
    "        y_pred = loj_model.predict(X)\n",
    "        print(confusion_matrix(y, y_pred))\n",
    "        print(\"accuracy: \" + str(accuracy_score(y, y_pred)))\n",
    "        print(classification_report(y, y_pred))\n",
    "        \n",
    "        #threshold değiştirilebilir\n",
    "        y_probs = loj_model.predict_proba(X)\n",
    "        y_pred = [1 if i > 0.5 else 0 for i in y_probs]\n",
    "        confusion_matrix(y, y_pred)\n",
    "        print(accuracy_score(y, y_pred))\n",
    "        print(classification_report(y, y_pred))\n",
    "        \n",
    "        #roc egrisi\n",
    "        logit_roc_auc = roc_auc_score(y, loj_model.predict(X))\n",
    "\n",
    "        fpr, tpr, thresholds = roc_curve(y, loj_model.predict_proba(X)[:,1])\n",
    "        plt.figure()\n",
    "        plt.plot(fpr, tpr, label='AUC (area = %0.2f)' % logit_roc_auc)\n",
    "        plt.plot([0, 1], [0, 1],'r--')\n",
    "        plt.xlim([0.0, 1.0])\n",
    "        plt.ylim([0.0, 1.05])\n",
    "        plt.xlabel('False Positive Oranı')\n",
    "        plt.ylabel('True Positive Oranı')\n",
    "        plt.title('ROC')\n",
    "        plt.show()\n",
    "        \n",
    "        \n",
    "        X_train, X_test, y_train, y_test =  train_test_split(X, y, test_size = 0.30, random_state = 42)\n",
    "        loj = LogisticRegression(solver = \"liblinear\")\n",
    "        loj_model = loj.fit(X_train,y_train)\n",
    "        print(accuracy_score(y_test, loj_model.predict(X_test)))\n",
    "        print(cross_val_score(loj_model, X_test, y_test, cv = 10).mean())\n",
    "        \n",
    "        \n",
    "    def Gaussian_Naive_Bayes(self, data, target):\n",
    "        print(\"\\n---------------------Gaussian Naive Bayes---------------------\")\n",
    "        from sklearn.naive_bayes import GaussianNB\n",
    "        X_train, X_test, y_train, y_test = self.Split(data, target)\n",
    "        nb = GaussianNB()\n",
    "        nb_model = nb.fit(X_train, y_train)\n",
    "        y_pred = nb_model.predict(X_test)\n",
    "        print(accuracy_score(y_test, y_pred))\n",
    "        print(cross_val_score(nb_model, X_test, y_test, cv = 10).mean())\n",
    "        \n",
    "    \n",
    "    def KNN(self, data, target):\n",
    "        print(\"\\n---------------------KNN---------------------\")\n",
    "        X_train, X_test, y_train, y_test = self.Split(data, target)\n",
    "        knn = KNeighborsClassifier()\n",
    "        knn_model = knn.fit(X_train, y_train)\n",
    "        y_pred = knn_model.predict(X_test)\n",
    "        print(accuracy_score(y_test, y_pred))\n",
    "        print(classification_report(y_test, y_pred))\n",
    "        \n",
    "        knn_params = {\"n_neighbors\": np.arange(1,50)}\n",
    "        knn = KNeighborsClassifier()\n",
    "        knn_cv = GridSearchCV(knn, knn_params, cv=10)\n",
    "        knn_cv.fit(X_train, y_train)\n",
    "        \n",
    "        print(\"En iyi skor:\" + str(knn_cv.best_score_))\n",
    "        print(\"En iyi parametreler: \" + str(knn_cv.best_params_))\n",
    "        knn = KNeighborsClassifier(knn_cv.best_params_)\n",
    "        knn_tuned = knn.fit(X_train, y_train)\n",
    "        print(knn_tuned.score(X_test, y_test))\n",
    "        y_pred = knn_tuned.predict(X_test)\n",
    "        print(accuracy_score(y_test, y_pred))\n",
    "        \n",
    "    \n",
    "    def SVC(self, data, target):\n",
    "        print(\"\\n---------------------SVC---------------------\")\n",
    "        X_train, X_test, y_train, y_test = self.Split(data, target)\n",
    "        svm_model = SVC(kernel = \"linear\").fit(X_train, y_train)\n",
    "        y_pred = svm_model.predict(X_test)\n",
    "        print(accuracy_score(y_test, y_pred))\n",
    "        \n",
    "        svc_params = {\"C\": np.arange(1,10)}\n",
    "        svc = SVC(kernel= \"linear\")\n",
    "        svc_cv_model = GridSearchCV(svc,svc_params, cv = 10, n_jobs = -1, verbose = 2 )\n",
    "        svc_cv_model.fit(X_train, y_train)\n",
    "        print(\"En iyi parametreler: \" + str(svc_cv_model.best_params_))\n",
    "        \n",
    "        svc_tuned = SVC(svc_cv_model.best_params_).fit(X_train, y_train)\n",
    "        y_pred = svc_tuned.predict(X_test)\n",
    "        print(accuracy_score(y_test, y_pred))\n",
    "    \n",
    "    \n",
    "    def CART(self, data, target):\n",
    "        from sklearn.tree import DecisionTreeClassifier\n",
    "        print(\"\\n---------------------CART---------------------\")\n",
    "        X_train, X_test, y_train, y_test = self.Split(data, target)\n",
    "        cart = DecisionTreeClassifier()\n",
    "        cart_model = cart.fit(X_train, y_train)\n",
    "        y_pred = cart_model.predict(X_test)\n",
    "        print(accuracy_score(y_test, y_pred))\n",
    "        \n",
    "        cart_grid = {\"max_depth\": range(1,10),\n",
    "            \"min_samples_split\" : list(range(2,50))}\n",
    "          \n",
    "        cart = tree.DecisionTreeClassifier()\n",
    "        cart_cv = GridSearchCV(cart, cart_grid, cv = 10, n_jobs = -1, verbose = 2)\n",
    "        cart_cv_model = cart_cv.fit(X_train, y_train)\n",
    "        print(\"En iyi parametreler: \" + str(cart_cv_model.best_params_))\n",
    "        cart = tree.DecisionTreeClassifier(max_depth = 5, min_samples_split = 19)\n",
    "        cart_tuned = cart.fit(X_train, y_train)\n",
    "        y_pred = cart_tuned.predict(X_test)\n",
    "        print(accuracy_score(y_test, y_pred))\n",
    "    \n",
    "    \n",
    "    def Random_Forests(self, data, target):\n",
    "        from sklearn.ensemble import RandomForestClassifier\n",
    "        print(\"\\n---------------------Random Forests---------------------\")\n",
    "        X_train, X_test, y_train, y_test = self.Split(data, target)\n",
    "        rf_model = RandomForestClassifier().fit(X_train, y_train)\n",
    "        y_pred = rf_model.predict(X_test)\n",
    "        print(accuracy_score(y_test, y_pred))\n",
    "        \n",
    "        rf_params = {\"max_depth\": [2,5,8,10],\n",
    "            \"max_features\": [2,5,8],\n",
    "            \"n_estimators\": [10,500,1000],\n",
    "            \"min_samples_split\": [2,5,10]}\n",
    "        rf_model = RandomForestClassifier()\n",
    "\n",
    "        rf_cv_model = GridSearchCV(rf_model, \n",
    "                           rf_params, \n",
    "                           cv = 10, \n",
    "                           n_jobs = -1, \n",
    "                           verbose = 2)\n",
    "        rf_cv_model.fit(X_train, y_train)\n",
    "        print(\"En iyi parametreler: \" + str(rf_cv_model.best_params_))\n",
    "        rf_tuned = RandomForestClassifier(rf_cv_model.best_params_)\n",
    "        rf_tuned.fit(X_train, y_train)\n",
    "        y_pred = rf_tuned.predict(X_test)\n",
    "        print(accuracy_score(y_test, y_pred))\n",
    "        \n",
    "        Importance = pd.DataFrame({\"Importance\": rf_tuned.feature_importances_*100},\n",
    "                         index = X_train.columns)\n",
    "        \n",
    "        Importance.sort_values(by = \"Importance\", axis = 0, ascending = True).plot(kind =\"barh\", color = \"r\")\n",
    "        plt.xlabel(\"Değişken Önem Düzeyleri\")\n",
    "        \n",
    "        \n",
    "    def neural_network(self, data, target):\n",
    "        from sklearn.preprocessing import StandardScaler \n",
    "        from sklearn.neural_network import MLPClassifier\n",
    "        print(\"\\n---------------------Neural_Network---------------------\")\n",
    "        X_train, X_test, y_train, y_test = self.Split(data, target)\n",
    "        scaler = StandardScaler()\n",
    "        scaler.fit(X_train)\n",
    "        X_train_scaled = scaler.transform(X_train)\n",
    "        X_test_scaled = scaler.transform(X_test)\n",
    "        mlpc = MLPClassifier().fit(X_train_scaled, y_train)\n",
    "        y_pred = mlpc.predict(X_test_scaled)\n",
    "        print(\"accuracy :\" + str(accuracy_score(y_test, y_pred))\n",
    "        mlpc_params = {\"alpha\": [0.1, 0.01, 0.02, 0.005, 0.0001,0.00001],\n",
    "              \"hidden_layer_sizes\": [(10,10,10),\n",
    "                                     (100,100,100),\n",
    "                                     (100,100),\n",
    "                                     (3,5), \n",
    "                                     (5, 3)],\n",
    "              \"solver\" : [\"lbfgs\",\"adam\",\"sgd\"],\n",
    "              \"activation\": [\"relu\",\"logistic\"]}\n",
    "              \n",
    "        mlpc = MLPClassifier()\n",
    "        mlpc_cv_model = GridSearchCV(mlpc, mlpc_params, \n",
    "                             cv = 10, \n",
    "                             n_jobs = -1,\n",
    "                             verbose = 2)\n",
    "\n",
    "        mlpc_cv_model.fit(X_train_scaled, y_train)\n",
    "        print(\"En iyi parametreler: \" + str(mlpc_cv_model.best_params_))\n",
    "        mlpc_tuned = MLPClassifier(mlpc_cv_model.best_params_)\n",
    "        mlpc_tuned.fit(X_train_scaled, y_train)\n",
    "        y_pred = mlpc_tuned.predict(X_test_scaled)\n",
    "        print(\"new accuracy: \" + str(accuracy_score(y_test, y_pred)))\n",
    "        \n",
    "    \n",
    "    def Gradient_Boosting_Machines(self, data, target):\n",
    "        from sklearn.ensemble import GradientBoostingClassifier\n",
    "        print(\"\\n---------------------Gradient Boosting Machines---------------------\")\n",
    "        X_train, X_test, y_train, y_test = self.Split(data, target)\n",
    "        gbm_model = GradientBoostingClassifier().fit(X_train, y_train)\n",
    "        y_pred = gbm_model.predict(X_test)\n",
    "        print(accuracy_score(y_test, y_pred))\n",
    "        gbm_params = {\"learning_rate\" : [0.001, 0.01, 0.1, 0.05],\n",
    "             \"n_estimators\": [100,500,100],\n",
    "             \"max_depth\": [3,5,10],\n",
    "             \"min_samples_split\": [2,5,10]}\n",
    "        gbm = GradientBoostingClassifier()\n",
    "        gbm_cv = GridSearchCV(gbm, gbm_params, cv = 10, n_jobs = -1, verbose = 2)\n",
    "        gbm_cv.fit(X_train, y_train)\n",
    "        print(\"En iyi parametreler: \" + str(gbm_cv.best_params_))\n",
    "        gbm = GradientBoostingClassifier(gbm_cv.best_params_)\n",
    "        gbm_tuned =  gbm.fit(X_train,y_train)\n",
    "        y_pred = gbm_tuned.predict(X_test)\n",
    "        print(accuracy_score(y_test, y_pred))\n",
    "    \n",
    "    \n",
    "    def XGBoost(self, data, target):\n",
    "        from xgboost import XGBClassifier\n",
    "        print(\"\\n---------------------XGBoost---------------------\")\n",
    "        X_train, X_test, y_train, y_test = self.Split(data, target)\n",
    "        xgb_model = XGBClassifier().fit(X_train, y_train)\n",
    "        y_pred = xgb_model.predict(X_test)\n",
    "        print(accuracy_score(y_test, y_pred))\n",
    "        \n",
    "        xgb_params = {\n",
    "        'n_estimators': [100, 500, 1000, 2000],\n",
    "        'subsample': [0.6, 0.8, 1.0],\n",
    "        'max_depth': [3, 4, 5,6],\n",
    "        'learning_rate': [0.1,0.01,0.02,0.05],\n",
    "        \"min_samples_split\": [2,5,10]}\n",
    "        \n",
    "        xgb = XGBClassifier()\n",
    "\n",
    "        xgb_cv_model = GridSearchCV(xgb, xgb_params, cv = 10, n_jobs = -1, verbose = 2)\n",
    "        xgb_cv_model.fit(X_train, y_train)\n",
    "        xgb = XGBClassifier(xgb_cv_model.best_params_)\n",
    "        xgb_tuned =  xgb.fit(X_train,y_train)\n",
    "        y_pred = xgb_tuned.predict(X_test)\n",
    "        print(accuracy_score(y_test, y_pred))\n",
    "        \n",
    "        \n",
    "    def CatBoost(self, data, target):\n",
    "        from catboost import CatBoostClassifier\n",
    "        print(\"\\n---------------------CatBoost---------------------\")\n",
    "        X_train, X_test, y_train, y_test = self.Split(data, target)\n",
    "        cat_model = CatBoostClassifier().fit(X_train, y_train)\n",
    "        y_pred = cat_model.predict(X_test)\n",
    "        print(accuracy_score(y_test, y_pred))\n",
    "        \n",
    "        catb_params = {\n",
    "            'iterations': [200,500],\n",
    "            'learning_rate': [0.01,0.05, 0.1],\n",
    "            'depth': [3,5,8] }\n",
    "        catb = CatBoostClassifier()\n",
    "        catb_cv_model = GridSearchCV(catb, catb_params, cv=5, n_jobs = -1, verbose = 2)\n",
    "        catb_cv_model.fit(X_train, y_train)\n",
    "        catb = CatBoostClassifier(catb_cv_model.best_params_)\n",
    "        catb_tuned = catb.fit(X_train, y_train)\n",
    "        y_pred = catb_tuned.predict(X_test)catb_cv_model.best_params_\n",
    "        y_pred = catb_tuned.predict(X_test)\n",
    "        print(accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 779,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataDefinition init çağrıldı\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_excel(\"HW_Data_Set.xlsx\")\n",
    "DataName=\"HW_Data_Set.xlsx\"\n",
    "df = data.copy()\n",
    "data_definition = DataDefinition(\"HW_Data_Set.xlsx\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
